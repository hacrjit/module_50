{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367123f2-eaeb-4c21-94d9-6b91d99f0fc8",
   "metadata": {},
   "source": [
    "### <b>Question No. 1</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d96c15-9fcb-4771-b260-e2ed2422aeb2",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical test used to compare the means of three or more groups to determine if there are statistically significant differences between them. ANOVA relies on several assumptions to be valid:\n",
    "\n",
    "1. **Independence**: Observations within and between groups are independent. This means that the data points in one group are not related to or influenced by the data points in another group.\n",
    "\n",
    "2. **Normality**: The data within each group are normally distributed. This assumption is important for the validity of the F-test used in ANOVA. However, ANOVA is robust to violations of normality if sample sizes are large enough.\n",
    "\n",
    "3. **Homogeneity of variances (Homoscedasticity)**: The variance of the data is the same across all groups (homogeneity of variances). This assumption is crucial for the validity of the F-test. Violations of this assumption can lead to inflated Type I error rates (false positives).\n",
    "\n",
    "4. **Interval or ratio scale**: The dependent variable should be measured on an interval or ratio scale. ANOVA is not appropriate for categorical or ordinal data.\n",
    "\n",
    "Examples of violations of these assumptions that could impact the validity of ANOVA results include:\n",
    "\n",
    "- **Non-normality**: If the data within groups are not normally distributed, the F-test may produce inaccurate results. For example, if the data are skewed or have heavy tails, the assumption of normality may be violated.\n",
    "\n",
    "- **Unequal variances**: If the variances of the groups are not equal (heteroscedasticity), the F-test may be unreliable. This can lead to incorrect conclusions about the differences between group means.\n",
    "\n",
    "- **Non-independence**: If observations are not independent, such as in repeated measures designs or clustered data, the assumptions of ANOVA are violated. In such cases, specialized ANOVA techniques (e.g., repeated measures ANOVA) or alternative tests may be more appropriate.\n",
    "\n",
    "- **Ordinal or categorical data**: ANOVA assumes that the dependent variable is measured on an interval or ratio scale. If the data are ordinal or categorical, ANOVA is not appropriate, and non-parametric tests (e.g., Kruskal-Wallis test) should be used instead.\n",
    "\n",
    "It's important to check these assumptions before interpreting the results of an ANOVA to ensure the validity and reliability of the conclusions drawn from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adaa4ac-d4a3-4b1c-b2b1-4b61f06b7c42",
   "metadata": {},
   "source": [
    "### <b>Question No. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c61724-c2a7-422d-81da-1ac43a8746a3",
   "metadata": {},
   "source": [
    "The three main types of ANOVA (Analysis of Variance) are:\n",
    "\n",
    "1. **One-way ANOVA**: This type of ANOVA is used when you have one independent variable with three or more levels (groups) and one dependent variable. It is used to determine if there are any statistically significant differences between the means of the groups. One-way ANOVA is appropriate when you are comparing the means of three or more independent groups. For example, you might use a one-way ANOVA to compare the effectiveness of three different treatments on a medical condition.\n",
    "\n",
    "2. **Two-way ANOVA**: This type of ANOVA is used when you have two independent variables (factors) and one dependent variable. It is used to determine if there are any interactions between the two independent variables and if each independent variable has a significant effect on the dependent variable. Two-way ANOVA is appropriate when you are interested in the effects of two categorical variables on a continuous outcome. For example, you might use a two-way ANOVA to investigate the effects of both gender and treatment on patient outcomes.\n",
    "\n",
    "3. **Repeated measures ANOVA**: This type of ANOVA is used when you have one group of participants and you measure the same dependent variable multiple times under different conditions. It is used to determine if there are any within-subjects effects (i.e., changes in the dependent variable over time or under different conditions) and if these effects are statistically significant. Repeated measures ANOVA is appropriate when you are interested in the effects of a manipulation that is applied repeatedly to the same participants. For example, you might use repeated measures ANOVA to assess the effects of different doses of a drug on blood pressure over time in the same group of participants.\n",
    "\n",
    "In summary, one-way ANOVA is used to compare the means of three or more independent groups, two-way ANOVA is used to examine the effects of two independent variables on a dependent variable, and repeated measures ANOVA is used to analyze data from repeated measurements of the same participants under different conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5cf8b-55ab-4eea-97b3-87d46b83114a",
   "metadata": {},
   "source": [
    "### <b>Question No. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff2069-9773-44ec-a4af-7d1ee6e591c3",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the division of the total variance in the dependent variable into different components that are attributable to different sources or factors. Understanding this concept is important because it helps to explain how the total variance in the data is accounted for by the independent variables in the model. The partitioning of variance allows us to quantify the amount of variance that is due to the effect of the independent variables and the amount that is due to random variability or error.\n",
    "\n",
    "In ANOVA, the total variance in the dependent variable is partitioned into three main components:\n",
    "\n",
    "1. **Between-group variance**: This component of variance represents the variability in the dependent variable that is due to differences between the group means. It is also known as the \"explained\" variance because it is attributed to the effect of the independent variable(s) on the dependent variable.\n",
    "\n",
    "2. **Within-group variance**: This component of variance represents the variability in the dependent variable that is not accounted for by the group means. It is also known as the \"unexplained\" or \"error\" variance because it is attributed to random variability or measurement error within each group.\n",
    "\n",
    "3. **Total variance**: This is the overall variability in the dependent variable across all observations. It is the sum of the between-group and within-group variances.\n",
    "\n",
    "By understanding the partitioning of variance in ANOVA, researchers can assess the relative importance of the independent variables in explaining the variability in the dependent variable. This understanding can help in interpreting the results of ANOVA and determining the significance of the effects of the independent variables. Additionally, the partitioning of variance is essential for calculating the F-statistic in ANOVA, which is used to test the null hypothesis of no significant difference between the group means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cf481-c324-4334-aa90-46319d1f9741",
   "metadata": {},
   "source": [
    "### <b>Question No. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47f66f-90e2-45d7-b902-5d1538a3be96",
   "metadata": {},
   "source": [
    "To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can use the following steps:\n",
    "\n",
    "1. Calculate the overall mean (grand mean) of the dependent variable.\n",
    "2. Calculate the total sum of squares (SST) by summing the squared differences between each observation and the overall mean.\n",
    "3. Calculate the explained sum of squares (SSE) by summing the squared differences between each group mean and the overall mean, weighted by the number of observations in each group.\n",
    "4. Calculate the residual sum of squares (SSR) by summing the squared differences between each observation and its group mean.\n",
    "\n",
    "Here's an example of how you can do this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab8919f-2146-4354-a84e-6c644333421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 40.0\n",
      "Explained Sum of Squares (SSE): 10.0\n",
      "Residual Sum of Squares (SSR): 30.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "group1 = [5, 7, 9, 6, 8]\n",
    "group2 = [4, 6, 8, 5, 7]\n",
    "group3 = [3, 5, 7, 4, 6]\n",
    "\n",
    "# Overall mean\n",
    "overall_mean = np.mean(group1 + group2 + group3)\n",
    "\n",
    "# Total sum of squares (SST)\n",
    "squared_diff_total = np.sum([(x - overall_mean)**2 for x in group1 + group2 + group3])\n",
    "\n",
    "# Explained sum of squares (SSE)\n",
    "group_means = [np.mean(group1), np.mean(group2), np.mean(group3)]\n",
    "squared_diff_group = np.sum([len(group) * (mean - overall_mean)**2 for group, mean in zip([group1, group2, group3], group_means)])\n",
    "\n",
    "# Residual sum of squares (SSR)\n",
    "squared_diff_residual = np.sum([(x - mean)**2 for group, mean in zip([group1, group2, group3], group_means) for x in group])\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", squared_diff_total)\n",
    "print(\"Explained Sum of Squares (SSE):\", squared_diff_group)\n",
    "print(\"Residual Sum of Squares (SSR):\", squared_diff_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98915e51-087e-43f4-9fa4-af8f6cfda9b1",
   "metadata": {},
   "source": [
    "In this example, `group1`, `group2`, and `group3` represent the data for each group in the one-way ANOVA. The code calculates the SST, SSE, and SSR based on these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e5aab-84d4-424c-8a66-1f95cdc99916",
   "metadata": {},
   "source": [
    "### <b>Question No. 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a1e80-ad14-4e94-89ec-0d4126ffe1b9",
   "metadata": {},
   "source": [
    "In a two-way ANOVA (Analysis of Variance), you can calculate the main effects and interaction effects using Python by fitting a linear model to your data. The `statsmodels` library provides a convenient way to perform ANOVA analysis. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4150025f-92db-48a3-8dc5-4b7475522af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effects:\n",
      "C(A)    1.868171\n",
      "C(B)    0.646030\n",
      "Name: F, dtype: float64\n",
      "\n",
      "Interaction Effect:\n",
      "0.19729063051053847\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create example data\n",
    "np.random.seed(0)\n",
    "data = {'A': np.random.choice(['A1', 'A2', 'A3'], 100),\n",
    "        'B': np.random.choice(['B1', 'B2'], 100),\n",
    "        'value': np.random.randn(100)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('value ~ C(A) + C(B) + C(A):C(B)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract main effects and interaction effects\n",
    "main_effects = anova_table.loc[['C(A)', 'C(B)'], 'F']\n",
    "interaction_effect = anova_table.loc['C(A):C(B)', 'F']\n",
    "\n",
    "print(\"Main Effects:\")\n",
    "print(main_effects)\n",
    "print(\"\\nInteraction Effect:\")\n",
    "print(interaction_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58687a8-6c8a-4b52-a61f-01297edb52a0",
   "metadata": {},
   "source": [
    "In this example, `C(A)` represents the main effect of variable A, `C(B)` represents the main effect of variable B, and `C(A):C(B)` represents the interaction effect between variables A and B. The `ols` function fits a linear model, and `sm.stats.anova_lm` computes the ANOVA table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d271c-61a0-406d-843e-5603f59a39cb",
   "metadata": {},
   "source": [
    "### <b>Question No. 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007814c-3e19-4fd4-aee4-5918af526063",
   "metadata": {},
   "source": [
    "In this scenario, a one-way ANOVA (Analysis of Variance) was conducted to analyze the differences between the means of three or more groups. The obtained F-statistic is 5.23, and the corresponding p-value is 0.02.\n",
    "\n",
    "1. **Interpreting the F-statistic**: The F-statistic tests the null hypothesis that all group means are equal. A high F-statistic indicates that the variance between group means is greater than the variance within groups, suggesting that there are significant differences between at least two of the group means.\n",
    "\n",
    "2. **Interpreting the p-value**: The p-value is the probability of observing an F-statistic as extreme as the one obtained, assuming that the null hypothesis is true. A p-value of 0.02 suggests that if there were no differences between the group means (i.e., the null hypothesis is true), you would observe an F-statistic as extreme as 5.23 only 2% of the time. Therefore, the low p-value indicates that the differences between the group means are statistically significant.\n",
    "\n",
    "3. **Conclusions**: Based on these results, you can conclude that there are statistically significant differences between at least two of the groups. However, the ANOVA does not tell you which specific groups are different from each other. Post-hoc tests, such as Tukey's HSD or Bonferroni correction, can be used to determine which groups differ significantly.\n",
    "\n",
    "Overall, the interpretation suggests that there are differences between the groups, but further analysis is needed to identify which groups differ significantly from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6998f-6a05-4195-8908-d5eee564f8dc",
   "metadata": {},
   "source": [
    "### <b>Question No. 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35340b-77de-4bde-a70b-59759ee7c013",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is important to ensure the validity of the analysis. There are several approaches to handling missing data:\n",
    "\n",
    "1. **Complete Case Analysis (CCA)**: This approach involves analyzing only the cases with complete data for all variables. While CCA is straightforward, it can lead to biased results if the missing data are not missing completely at random (MCAR), as it may exclude informative data points.\n",
    "\n",
    "2. **Mean Imputation**: Missing values are replaced with the mean of the observed values for that variable. While this method is simple, it can underestimate the standard errors and can distort the relationships in the data.\n",
    "\n",
    "3. **Last Observation Carried Forward (LOCF)**: Missing values are replaced with the last observed value. LOCF is simple but can lead to biased estimates, especially if there is a trend in the data.\n",
    "\n",
    "4. **Multiple Imputation (MI)**: Missing values are imputed multiple times to create several complete datasets, and the analyses are performed on each dataset. The results are then combined to provide more accurate estimates. MI is more complex but can provide more reliable results if the imputation model is correctly specified.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include biased estimates, underestimation of variability, and incorrect conclusions. It is important to carefully consider the nature of the missing data and choose an appropriate method that minimizes bias and preserves the integrity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688e988-9b69-4600-8848-79f440c03fa4",
   "metadata": {},
   "source": [
    "### <b>Question No. 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f4bb5-0c42-4a3f-9f99-301bfe9113b7",
   "metadata": {},
   "source": [
    "Common post-hoc tests used after ANOVA include Tukey's Honestly Significant Difference (HSD) test, the Bonferroni correction, and the Scheffe test. These tests are used to determine which specific groups differ significantly from each other after finding a significant result in the ANOVA.\n",
    "\n",
    "1. **Tukey's HSD**: Tukey's test is used when you have a specific hypothesis about which groups might differ from each other. It controls the family-wise error rate, which is the probability of making at least one Type I error (false positive) in a set of comparisons.\n",
    "\n",
    "2. **Bonferroni Correction**: The Bonferroni correction adjusts the significance level for each individual comparison to control the overall family-wise error rate. It is more conservative than Tukey's test but is appropriate when you have no specific hypotheses about which groups might differ.\n",
    "\n",
    "3. **Scheffe Test**: The Scheffe test is a more conservative post-hoc test that can be used when the assumptions of other tests are not met, such as unequal group sizes or unequal variances.\n",
    "\n",
    "Example: Suppose you conducted an ANOVA to compare the mean scores of three different teaching methods (A, B, and C) on student performance. The ANOVA results indicate a significant difference between the teaching methods. To determine which teaching methods are significantly different from each other, you would conduct a post-hoc test, such as Tukey's HSD, to compare all possible pairs of teaching methods. This would help identify the specific teaching methods that lead to significantly different student performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad797c-450f-4299-9fec-de6a512549a8",
   "metadata": {},
   "source": [
    "### <b>Question No. 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f547d-96ae-4b70-bddd-ab0ce59a5875",
   "metadata": {},
   "source": [
    "To conduct a one-way ANOVA in Python to compare the mean weight loss of three diets (A, B, and C), you can use the `scipy.stats` module. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0cc34fc-10a2-4e95-8aea-e91351ea4482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 9.734159327859446\n",
      "p-value: 0.00010714009025069481\n",
      "There is a significant difference between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Generate example weight loss data for each diet\n",
    "np.random.seed(42)  # for reproducibility\n",
    "weight_loss_a = np.random.normal(5, 1, 50)  # mean=5, std=1\n",
    "weight_loss_b = np.random.normal(4.5, 1, 50)  # mean=4.5, std=1\n",
    "weight_loss_c = np.random.normal(4, 1, 50)  # mean=4, std=1\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_a, weight_loss_b, weight_loss_c)\n",
    "\n",
    "# Print results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6141681-f03f-4c07-9a73-8c0324614533",
   "metadata": {},
   "source": [
    "In this example, we generate example weight loss data for each diet using normal distributions with different means (5, 4.5, and 4) and a standard deviation of 1. Then, we use `f_oneway` from `scipy.stats` to perform the one-way ANOVA and obtain the F-statistic and p-value. Finally, we interpret the results based on the p-value compared to a significance level (alpha) of 0.05. If the p-value is less than alpha, we conclude that there is a significant difference between the mean weight loss of the three diets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb4c05-9a5d-40be-8b95-e1887fe055d8",
   "metadata": {},
   "source": [
    "### <b>Question No. 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c0fc4-059c-498e-bfb9-bb4472a7063e",
   "metadata": {},
   "source": [
    "To conduct a two-way ANOVA in Python for the given scenario, you can use the `statsmodels` library. First, you'll need to simulate some data to demonstrate the analysis. Let's assume we have data in the following format:\n",
    "\n",
    "- Software Program (A, B, C)\n",
    "- Employee Experience Level (Novice, Experienced)\n",
    "- Task Completion Time\n",
    "\n",
    "Here's how you can conduct the two-way ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d7369b-f7ca-46b8-9dec-22d0240a1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       sum_sq    df         F    PR(>F)\n",
      "Program             11.141545   2.0  2.113814  0.142706\n",
      "Experience           2.102143   1.0  0.797652  0.380665\n",
      "Program:Experience   6.013261   2.0  1.140857  0.336272\n",
      "Residual            63.249921  24.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "n = 30\n",
    "programs = np.random.choice(['A', 'B', 'C'], n)\n",
    "experience = np.random.choice(['Novice', 'Experienced'], n)\n",
    "time = np.random.normal(loc=10, scale=2, size=n)  # Mean time of 10, standard deviation of 2\n",
    "\n",
    "data = pd.DataFrame({'Program': programs, 'Experience': experience, 'Time': time})\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('Time ~ Program + Experience + Program:Experience', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe5f2a-9cc8-4c2a-b06d-78873ffa2528",
   "metadata": {},
   "source": [
    "This code first simulates some data for the three programs, novice and experienced employees, and task completion times. It then fits a two-way ANOVA model and prints the ANOVA table containing the F-statistics and p-values for the main effects of the software programs and employee experience level, as well as the interaction effect between them. The interpretation of the results would involve looking at the p-values to determine if there are any significant effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028bd07-a6b4-465a-b1e3-70b7749e3b7d",
   "metadata": {},
   "source": [
    "### <b>Question No. 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96eb5c-a9ed-45ce-b88f-54599cf4435d",
   "metadata": {},
   "source": [
    "To conduct a two-sample t-test in Python to compare the test scores between the control group (traditional teaching method) and the experimental group (new teaching method), you can use the `scipy.stats` module. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33554888-8ad5-4642-9eee-e48319e571a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -3.597192759749614\n",
      "p-value: 0.0004062796020362504\n",
      "   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "=========================================================\n",
      " group1    group2    meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------------\n",
      "Control Experimental    5.222 0.0004 2.3593 8.0848   True\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(0)\n",
    "control_scores = np.random.normal(loc=70, scale=10, size=100)\n",
    "experimental_scores = np.random.normal(loc=75, scale=10, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Perform post-hoc test (e.g., Tukey's HSD)\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "data = np.concatenate([control_scores, experimental_scores])\n",
    "group = ['Control'] * 100 + ['Experimental'] * 100\n",
    "\n",
    "mc = MultiComparison(data, group)\n",
    "result = mc.tukeyhsd()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4912d2-f3b4-47aa-9fe6-9fda6e034f3d",
   "metadata": {},
   "source": [
    "In this code, we first simulate test scores for the control and experimental groups using normal distributions with means of 70 and 75, and standard deviations of 10 for both groups. We then use the `ttest_ind` function from `scipy.stats` to perform the two-sample t-test and calculate the t-statistic and p-value.\n",
    "\n",
    "If the p-value is less than the chosen significance level (e.g., 0.05), we reject the null hypothesis and conclude that there is a significant difference in test scores between the two groups.\n",
    "\n",
    "If the results are significant, you can follow up with a post-hoc test to determine which group(s) differ significantly from each other. In this example, we used Tukey's Honestly Significant Difference (HSD) test, which is available in the `statsmodels` library. The post-hoc test helps identify which specific group(s) differ significantly from each other after finding a significant result in the overall test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582824c-7385-4288-93ca-c0995889c9a7",
   "metadata": {},
   "source": [
    "### <b>Question No. 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ccf24ba-2eeb-4250-94c0-c1bb39c24bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source  ddof1  ddof2         F     p-unc       ng2       eps\n",
      "0    Day     29     58  0.861614  0.662935  0.299302  0.065595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "\n",
    "# Create a DataFrame with the sales data\n",
    "np.random.seed(0)\n",
    "days = list(range(1, 31))\n",
    "store_a_sales = np.random.normal(100, 10, 30)\n",
    "store_b_sales = np.random.normal(110, 15, 30)\n",
    "store_c_sales = np.random.normal(105, 12, 30)\n",
    "\n",
    "data = {\n",
    "    'Day': days * 3,\n",
    "    'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30,\n",
    "    'Sales': list(store_a_sales) + list(store_b_sales) + list(store_c_sales)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Conduct repeated measures ANOVA\n",
    "aov = pg.rm_anova(dv='Sales', within='Day', subject='Store', data=df)\n",
    "print(aov)\n",
    "\n",
    "# Conduct post-hoc test if the ANOVA result is significant\n",
    "if aov['p-unc'][0] < 0.05:\n",
    "    posthoc = pg.pairwise_ttests(dv='Sales', within='Day', subject='Store', data=df, parametric=True, padjust='bonf')\n",
    "    print(posthoc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
